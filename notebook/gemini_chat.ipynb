{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc092ea5",
   "metadata": {},
   "source": [
    "# Gemini Chat demo\n",
    "This notebook demonstrates calling a Gemini chat LLM from a workspace project, securely loading keys from `.env`, a client init, a REST fallback, a reusable chat wrapper, retries, and an example conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ea0536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement google-generative-ai (from versions: none)\n",
      "ERROR: No matching distribution found for google-generative-ai\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "%pip install -q langchain-google-genai google-generative-ai python-dotenv jupyter ipykernel requests pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e48f3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Ensure langchain is installed (notebook-scoped)\n",
    "%pip install -q langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc45a23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment and project config\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(\"GOOGLE_API_KEY loaded:\", bool(GOOGLE_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9627ea09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured google.generativeai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\4188828679.py:3: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  from google import generativeai as genai\n"
     ]
    }
   ],
   "source": [
    "# Try configuring official google generative client if available\n",
    "try:\n",
    "    from google import generativeai as genai\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    print(\"Configured google.generativeai\")\n",
    "except Exception as e:\n",
    "    print(\"google.generativeai not available or failed to configure:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdd08c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REST call failed (this can be normal if API key or model name requires OAuth): 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta2/models/chat-bison-001:generateMessage?key=AIzaSyCg065RDAddocUNICWEc31oMA4a91Rc_ic\n"
     ]
    }
   ],
   "source": [
    "# Fallback: raw REST call to Gemini-like endpoint using requests\n",
    "import requests, json\n",
    "\n",
    "def call_gemini_rest(prompt, model=\"chat-bison-001\"):\n",
    "    url = f\"https://generativelanguage.googleapis.com/v1beta2/models/{model}:generateMessage?key={GOOGLE_API_KEY}\"\n",
    "    body = {\"messages\": [{\"author\": \"user\", \"content\": prompt}], \"temperature\": 0.2}\n",
    "    resp = requests.post(url, json=body, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()\n",
    "\n",
    "# Quick test (may fail depending on auth method)\n",
    "try:\n",
    "    small_test = call_gemini_rest(\"Say hello and list 3 travel tips for train travel in India.\")\n",
    "    print(json.dumps(small_test, indent=2)[:1000])\n",
    "except Exception as e:\n",
    "    print(\"REST call failed (this can be normal if API key or model name requires OAuth):\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b948eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat wrapper with retries, streaming placeholder, and logging\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def chat_sync(prompt, model=\"chat-bison-001\", max_retries=3):\n",
    "    last_err = None\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            # Try using official client if configured\n",
    "            if 'genai' in globals():\n",
    "                try:\n",
    "                    resp = genai.chat.create(model=model, messages=[{\"author\":\"user\",\"content\":prompt}])\n",
    "                    # genai response shapes vary; convert to dict if possible\n",
    "                    return dict(resp)\n",
    "                except Exception as inner_e:\n",
    "                    last_err = inner_e\n",
    "                    # fallthrough to REST fallback\n",
    "            # REST fallback\n",
    "            resp = call_gemini_rest(prompt, model=model)\n",
    "            return resp\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            backoff = 2 ** attempt\n",
    "            print(f\"Attempt {attempt} failed, retrying in {backoff}s: {e}\")\n",
    "            time.sleep(backoff)\n",
    "    raise RuntimeError(f\"All retries failed: {last_err}\")\n",
    "\n",
    "# Save conversation utility\n",
    "def save_transcript(prompt, response, out_dir='.', prefix='gemini_convo'):\n",
    "    ts = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n",
    "    path = f\"{out_dir}/{prefix}_{ts}.json\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump({'prompt': prompt, 'response': response}, f, ensure_ascii=False, indent=2)\n",
    "    print('Saved transcript to', path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae5af7f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed, retrying in 2s: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta2/models/chat-bison-001:generateMessage?key=AIzaSyCg065RDAddocUNICWEc31oMA4a91Rc_ic\n",
      "Attempt 2 failed, retrying in 4s: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta2/models/chat-bison-001:generateMessage?key=AIzaSyCg065RDAddocUNICWEc31oMA4a91Rc_ic\n",
      "Attempt 3 failed, retrying in 8s: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta2/models/chat-bison-001:generateMessage?key=AIzaSyCg065RDAddocUNICWEc31oMA4a91Rc_ic\n",
      "Conversation failed: All retries failed: 400 Client Error: Bad Request for url: https://generativelanguage.googleapis.com/v1beta2/models/chat-bison-001:generateMessage?key=AIzaSyCg065RDAddocUNICWEc31oMA4a91Rc_ic\n"
     ]
    }
   ],
   "source": [
    "# Example conversation\n",
    "prompt = \"Say hello and list 3 travel tips for train travel in India.\"\n",
    "try:\n",
    "    result = chat_sync(prompt, model=\"chat-bison-001\")\n",
    "    print('Raw response snippet:')\n",
    "    import json\n",
    "    print(json.dumps(result, indent=2)[:2000])\n",
    "    save_transcript(prompt, result)\n",
    "except Exception as e:\n",
    "    print('Conversation failed:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cba117",
   "metadata": {},
   "source": [
    "# Tests & notes\n",
    "\n",
    "- To run tests: `pytest -q` (tests would mock network calls).\n",
    "- This notebook includes a streaming placeholder â€” actual streaming depends on official client support.\n",
    "- Do not commit `.env` to source control; add it to `.gitignore`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fb42f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root added to sys.path: c:\\Irctc_POC\n",
      "LLM instance: <class 'langchain_groq.chat_models.ChatGroq'> profile={'max_input_tokens': 131072, 'max_output_tokens': 32768, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True} client=<groq.resources.chat.completions.Completions object at 0x000001C5870DACF0> async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001C5870DB500> model_name='llama-3.3-70b-versatile' temperature=1e-08 model_kwargs={} groq_api_key=SecretStr('**********') max_retries=3\n",
      "Messages call failed: No module named 'langchain.schema'\n",
      "\n",
      "Trying llm.predict with string...\n",
      "predict failed: 'ChatGroq' object has no attribute 'predict'\n",
      "\n",
      "Trying llm.generate with messages (list of message-lists)...\n",
      "generate failed: name 'HumanMessage' is not defined\n",
      "Saved transcript to ./gemini_convo_20260221T141011Z.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\4059928304.py\", line 17, in <module>\n",
      "    from langchain.schema import HumanMessage\n",
      "ModuleNotFoundError: No module named 'langchain.schema'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\4059928304.py\", line 29, in <module>\n",
      "    resp = llm.predict(prompt)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\pydantic\\main.py\", line 1026, in __getattr__\n",
      "    raise AttributeError(f'{type(self).__name__!r} object has no attribute {item!r}')\n",
      "AttributeError: 'ChatGroq' object has no attribute 'predict'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\4059928304.py\", line 37, in <module>\n",
      "    resp = llm.generate([[HumanMessage(content=prompt)]])\n",
      "                          ^^^^^^^^^^^^\n",
      "NameError: name 'HumanMessage' is not defined\n",
      "C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\2586164401.py:31: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  ts = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n"
     ]
    }
   ],
   "source": [
    "# Demo: use the LangChain wrapper from app.llm.model and try common call patterns\n",
    "import sys, pathlib\n",
    "repo_root = pathlib.Path.cwd().parent\n",
    "if str(repo_root) not in sys.path:\n",
    "    sys.path.insert(0, str(repo_root))\n",
    "print('Repo root added to sys.path:', repo_root)\n",
    "\n",
    "from app.llm.model import get_llm\n",
    "llm = get_llm()\n",
    "print('LLM instance:', type(llm), llm)\n",
    "\n",
    "prompt = \"Say hello and list 3 travel tips for train travel in India.\"\n",
    "import traceback\n",
    "\n",
    "# Try calling with HumanMessage list\n",
    "try:\n",
    "    from langchain.schema import HumanMessage\n",
    "    print('\\nTrying callable with messages (HumanMessage)...')\n",
    "    resp = llm([HumanMessage(content=prompt)])\n",
    "    print('Messages call result type:', type(resp))\n",
    "    print(resp)\n",
    "except Exception as e:\n",
    "    print('Messages call failed:', e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Try predict or generate variants\n",
    "try:\n",
    "    print('\\nTrying llm.predict with string...')\n",
    "    resp = llm.predict(prompt)\n",
    "    print('predict result:', resp)\n",
    "except Exception as e:\n",
    "    print('predict failed:', e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "try:\n",
    "    print('\\nTrying llm.generate with messages (list of message-lists)...')\n",
    "    resp = llm.generate([[HumanMessage(content=prompt)]])\n",
    "    print('generate result:', resp)\n",
    "except Exception as e:\n",
    "    print('generate failed:', e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Save transcript placeholder\n",
    "try:\n",
    "    save_transcript(prompt, {'note': 'LangChain demo executed'})\n",
    "except Exception as e:\n",
    "    print('save_transcript failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7816925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying llm.generate with DummyMessage wrapper\n",
      "generate with DummyMessage failed: Got unknown type <__main__.DummyMessage object at 0x000001C5FBC73470>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\shiva\\AppData\\Local\\Temp\\ipykernel_22876\\890835290.py\", line 8, in <module>\n",
      "    resp = llm.generate([[DummyMessage(prompt)]])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 933, in generate\n",
      "    self._generate_with_cache(\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py\", line 1235, in _generate_with_cache\n",
      "    result = self._generate(\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 616, in _generate\n",
      "    message_dicts, params = self._create_message_dicts(messages, stop)\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 810, in _create_message_dicts\n",
      "    message_dicts = [_convert_message_to_dict(m) for m in messages]\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\shiva\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\yes-hv3lZ-Qa-py3.12\\Lib\\site-packages\\langchain_groq\\chat_models.py\", line 1377, in _convert_message_to_dict\n",
      "    raise TypeError(msg)\n",
      "TypeError: Got unknown type <__main__.DummyMessage object at 0x000001C5FBC73470>\n"
     ]
    }
   ],
   "source": [
    "# Construct a minimal message-like object and try generate\n",
    "class DummyMessage:\n",
    "    def __init__(self, content):\n",
    "        self.content = content\n",
    "\n",
    "try:\n",
    "    print('Trying llm.generate with DummyMessage wrapper')\n",
    "    resp = llm.generate([[DummyMessage(prompt)]])\n",
    "    print('generate result:', resp)\n",
    "except Exception as e:\n",
    "    print('generate with DummyMessage failed:', e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "65241aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client.create returned type: <class 'groq.types.chat.chat_completion.ChatCompletion'>\n",
      "response snippet (truncated): {\"id\": \"chatcmpl-c05f9f02-3467-44e5-a1ff-eff9dfaa2ae0\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"Hello! If you're planning to travel by train in India, here are three helpful tips to keep in mind:\\n\\n1. **Book your tickets in advance**: India has a vast rail network, and trains can get very crowded, especially during peak travel seasons. Booking your tickets well in advance can help you secure a seat or berth, and also give you a better chance of getting a preference for your seat location.\\n\\n2. **Be prepared for delays**: Train travel in India can be unpredictable, and delays are common. Make sure to check the train's status before your journey, and be prepared for unexpected delays by carrying snacks, water, and entertainment for the journey.\\n\\n3. **Choose your class wisely**: Indian trains have various classes, ranging from the basic Second Class (also known as General Class) to the more luxurious First Class AC. Choose a class th\n"
     ]
    }
   ],
   "source": [
    "# Retry groq client call with role-based messages (content as string) and print a short excerpt\n",
    "try:\n",
    "    payload = [{\"role\":\"user\",\"content\": prompt}]\n",
    "    resp = llm.client.create(model=llm.model_name, messages=payload)\n",
    "    print('client.create returned type:', type(resp))\n",
    "    try:\n",
    "        d = resp.to_dict()\n",
    "        import json\n",
    "        s = json.dumps(d)[:1000]\n",
    "        print('response snippet (truncated):', s)\n",
    "    except Exception:\n",
    "        print('Could not convert response to dict; repr limited:', repr(resp)[:1000])\n",
    "except Exception as e:\n",
    "    print('client.create failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1eeabd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instantiated LLM (preferred google): <class 'langchain_google_genai.chat_models.ChatGoogleGenerativeAI'>\n",
      "Calling wrapper directly...\n",
      "Wrapper direct call failed: No module named 'langchain.schema'\n"
     ]
    }
   ],
   "source": [
    "# Reload `app.llm.model` and instantiate preferred LLM (Google preferred)\n",
    "import importlib\n",
    "import app.llm.model as model_mod\n",
    "importlib.reload(model_mod)\n",
    "llm_google_pref = model_mod.get_llm(prefer_provider='google')\n",
    "print('Instantiated LLM (preferred google):', type(llm_google_pref))\n",
    "\n",
    "# If it's the Google wrapper, try a simple direct call using the client's create (safe shape)\n",
    "prompt = \"Say hello and list 3 travel tips for train travel in India.\"\n",
    "try:\n",
    "    # many wrappers expose `.client` - if present, use it with a safe payload\n",
    "    if hasattr(llm_google_pref, 'client') and hasattr(llm_google_pref.client, 'create'):\n",
    "        print('Using underlying client.create for Google wrapper...')\n",
    "        resp = llm_google_pref.client.create(model=getattr(llm_google_pref, 'model', 'gemini-2.0-flash'), messages=[{\"role\":\"user\",\"content\": prompt}])\n",
    "        print('client.create response snippet:', str(resp)[:1000])\n",
    "    else:\n",
    "        # fallback to calling the wrapper directly\n",
    "        print('Calling wrapper directly...')\n",
    "        try:\n",
    "            from langchain.schema import HumanMessage\n",
    "            out = llm_google_pref([HumanMessage(content=prompt)])\n",
    "            print('Wrapper call output:', out)\n",
    "        except Exception as e:\n",
    "            print('Wrapper direct call failed:', e)\n",
    "except Exception as e:\n",
    "    print('Google-pref LLM call failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "13351c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM type: <class 'langchain_groq.chat_models.ChatGroq'>\n",
      "\n",
      "Model reply (excerpt):\n",
      "Hello! If you're planning to travel by train in India, here are 3 useful tips to keep in mind:\n",
      "\n",
      "1. **Book tickets in advance**: Train travel is a popular mode of transportation in India, and tickets can sell out quickly, especially during peak travel seasons. Make sure to book your tickets well in advance to secure your seat.\n",
      "\n",
      "2. **Choose the right class**: Indian Railways offers various classes of travel, ranging from the basic Second Class (also known as General or Unreserved) to the more luxurious First Class AC. Consider your budget and preferences when choosing your class, and be aware that higher classes often offer more comfort and amenities.\n",
      "\n",
      "3. **Pack wisely and stay safe**: Train travel in India can be crowded and chaotic, so it's essential to pack light and keep your belongings secure. Bring a money belt or a secure bag to carry your valuables, and be mindful of your surroundings, especially in crowded stations or trains. Also, consider carrying a water bottle, snacks, and a portable charger to stay comfortable during your journey.\n",
      "\n",
      "I hope these tips are helpful for your train travel adventure in India!\n"
     ]
    }
   ],
   "source": [
    "# Simple: call Groq-backed LLM and print reply\n",
    "from app.llm.model import get_llm\n",
    "llm_groq = get_llm(prefer_provider='groq')\n",
    "print('LLM type:', type(llm_groq))\n",
    "\n",
    "prompt = \"Say hello and list 3 travel tips for train travel in India.\"\n",
    "try:\n",
    "    # Use underlying groq client for a straightforward call\n",
    "    payload = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    resp = llm_groq.client.create(model=llm_groq.model_name, messages=payload)\n",
    "    # Safe extraction: convert to dict if possible and show main message text\n",
    "    d = resp.to_dict() if hasattr(resp, 'to_dict') else resp\n",
    "    # Try to drill into choices -> message -> content\n",
    "    text = None\n",
    "    try:\n",
    "        choices = d.get('choices') if isinstance(d, dict) else None\n",
    "        if choices and len(choices) > 0:\n",
    "            message = choices[0].get('message')\n",
    "            if isinstance(message, dict):\n",
    "                content = message.get('content')\n",
    "                if isinstance(content, str):\n",
    "                    text = content\n",
    "                else:\n",
    "                    # content may be a nested object\n",
    "                    text = str(content)\n",
    "    except Exception:\n",
    "        text = str(d)[:1000]\n",
    "    print('\\nModel reply (excerpt):')\n",
    "    print(text or str(d)[:1000])\n",
    "except Exception as e:\n",
    "    print('Groq call failed:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd939a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! India is a vibrant country with a vast and extensive rail network, making train travel a popular and exciting way to explore the country. Here are three travel tips for train travel in India:\n",
      "\n",
      "1. **Book your tickets in advance**: Train travel in India can be very popular, and trains often get fully booked, especially during peak travel seasons. It's essential to book your tickets well in advance to ensure availability and to get the best prices.\n",
      "\n",
      "2. **Choose your class wisely**: Indian trains have various classes, ranging from luxurious air-conditioned cars to more basic, non-air-conditioned options. Consider your budget and comfort level when choosing your class, and be aware that different classes may have different amenities and services.\n",
      "\n",
      "3. **Be prepared for crowds and delays**: Train travel in India can be unpredictable, with crowds and delays being common. Be patient and flexible, and consider packing snacks, water, and entertainment for your journey. Also, be mindful of \n"
     ]
    }
   ],
   "source": [
    "# Quick Groq test via `llm_chat`\n",
    "from app.services import llm_chat\n",
    "resp = llm_chat(\"Say hello and list 3 travel tips for train travel in India.\", provider=\"groq\", save=False)\n",
    "print(resp.get('text')[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b244a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ present: True\n",
      "GROQ len: 56\n",
      "GROQ mask: ***f8ddex\n"
     ]
    }
   ],
   "source": [
    "# Check GROQ_API_KEY present in notebook\n",
    "import os\n",
    "k = os.getenv('GROQ_API_KEY')\n",
    "print('GROQ present:', bool(k))\n",
    "print('GROQ len:', len(k or ''))\n",
    "if k:\n",
    "    print('GROQ mask:', '***' + k[-6:])\n",
    "else:\n",
    "    print('GROQ mask: NOT SET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1794d9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded GROQ mask: ***f8ddex\n",
      "Call result: Hello. For train travel in India, consider booking an upper berth for a more peaceful and secure journey, as it's generally less crowded and quieter than lower berths.\n"
     ]
    }
   ],
   "source": [
    "# Reload .env fresh (force dotenv reload)\n",
    "import sys\n",
    "import os\n",
    "# Remove cached config module\n",
    "if 'app.config' in sys.modules:\n",
    "    del sys.modules['app.config']\n",
    "# Force reload of .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)  # Override any existing env vars with .env content\n",
    "k = os.getenv('GROQ_API_KEY')\n",
    "print('Reloaded GROQ mask:', ('***' + k[-6:]) if k else 'NOT SET')\n",
    "# Try call again\n",
    "from app.services import llm_chat\n",
    "resp = llm_chat(\"Briefly say hello and give one travel tip for train travel in India.\", provider=\"groq\", save=False)\n",
    "print('Call result:', resp['text'][:500])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yes-hv3lZ-Qa-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
